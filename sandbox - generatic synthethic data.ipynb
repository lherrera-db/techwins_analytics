{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9938e29-effa-4e91-b3e2-b742f3adf87c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the execution timeout to 4 hours (14400 seconds)\n",
    "# Note: On serverless notebooks, the default timeout is 2.5 hours. \n",
    "# You can manually set a longer timeout (e.g., 4 hours) using this config, \n",
    "# but actual enforcement may depend on workspace and cluster policies.\n",
    "spark.conf.set(\"spark.databricks.execution.timeout\", \"14400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e601e753-7c61-43f2-b6d8-45e787162349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbldatagen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb82296-a9d5-45d7-bf5f-cbc60a0f72c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf30bf5-6e35-4e75-9f42-78dd8a31d989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get schema from the existing table\n",
    "table_name = \"main.gtm_data.core_usecase_curated\"\n",
    "df = spark.table(table_name)\n",
    "schema = df.schema\n",
    "\n",
    "display(df.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8927e976-0353-4d25-a35a-4377355f4c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Extract unique values for subregion and sales fields\n",
    "sales_region_values = [row[0] for row in df.select(\"sales_region\").distinct().collect() if row[0] is not None]\n",
    "sales_subregion1_values = [row[0] for row in df.select(\"sales_subregion_level_1\").distinct().collect() if row[0] is not None]\n",
    "sales_subregion2_values = [row[0] for row in df.select(\"sales_subregion_level_2\").distinct().collect() if row[0] is not None]\n",
    "sales_subregion3_values = [row[0] for row in df.select(\"sales_subregion_level_3\").distinct().collect() if row[0] is not None]\n",
    "account_exec_values = [row[0] for row in df.select(\"account_executive\").distinct().collect() if row[0] is not None]\n",
    "sales_mgr_values = [row[0] for row in df.select(\"sales_manager\").distinct().collect() if row[0] is not None]\n",
    "solution_arch_values = [row[0] for row in df.select(\"solution_architect\").distinct().collect() if row[0] is not None]\n",
    "\n",
    "# Get sample values for copy-over fields\n",
    "demand_plan_stage_next_steps_values = [row[0] for row in df.select(\"demand_plan_stage_next_steps\").distinct().collect() if row[0] is not None]\n",
    "usecase_description_values = [row[0] for row in df.select(\"usecase_description\").distinct().collect() if row[0] is not None]\n",
    "implementation_notes_values = [row[0] for row in df.select(\"implementation_notes\").distinct().collect() if row[0] is not None]\n",
    "business_impact_values = [row[0] for row in df.select(\"business_impact\").distinct().collect() if row[0] is not None]\n",
    "sdr_bdr_notes_values = [row[0] for row in df.select(\"sdr_bdr_notes\").distinct().collect() if row[0] is not None]\n",
    "\n",
    "# Compute min/max for all date and timestamp fields\n",
    "date_ranges = {}\n",
    "for field in schema:\n",
    "    if field.dataType.typeName() in [\"date\", \"timestamp\"]:\n",
    "        minmax = df.agg({field.name: \"min\"}).collect()[0][0], df.agg({field.name: \"max\"}).collect()[0][0]\n",
    "        if minmax[0] is not None and minmax[1] is not None:\n",
    "            date_ranges[field.name] = minmax\n",
    "\n",
    "row_count = 1000  # Set desired number of synthetic rows\n",
    "\n",
    "spec = dg.DataGenerator(spark, name=\"synthetic_core_usecase_curated\", rows=row_count, partitions=4)\n",
    "\n",
    "for field in schema:\n",
    "    col_name = field.name\n",
    "    col_type = field.dataType\n",
    "    nullable = field.nullable\n",
    "\n",
    "    if col_name == \"sales_region\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=sales_region_values, nullable=nullable)\n",
    "    elif col_name == \"sales_subregion_level_1\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=sales_subregion1_values, nullable=nullable)\n",
    "    elif col_name == \"sales_subregion_level_2\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=sales_subregion2_values, nullable=nullable)\n",
    "    elif col_name == \"sales_subregion_level_3\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=sales_subregion3_values, nullable=nullable)\n",
    "    elif col_name == \"subregion_1\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=subregion1_values, nullable=nullable)\n",
    "    elif col_name == \"subregion_2\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=subregion2_values, nullable=nullable)\n",
    "    elif col_name == \"subregion_3\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=subregion3_values, nullable=nullable)\n",
    "    elif col_name == \"demand_plan_stage_next_steps\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=demand_plan_stage_next_steps_values, nullable=nullable)\n",
    "    elif col_name == \"usecase_description\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=usecase_description_values, nullable=nullable)\n",
    "    elif col_name == \"implementation_notes\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=implementation_notes_values, nullable=nullable)\n",
    "    elif col_name == \"business_impact\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=business_impact_values, nullable=nullable)\n",
    "    elif col_name == \"sdr_bdr_notes\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=sdr_bdr_notes_values, nullable=nullable)\n",
    "    elif col_name == \"account_executive\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=account_exec_values, nullable=nullable)\n",
    "    elif col_name == \"field_manager\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=field_mgr_values, nullable=nullable)\n",
    "    elif col_name == \"sales_manager\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=sales_mgr_values, nullable=nullable)\n",
    "    elif col_name == \"solution_architect\":\n",
    "        spec = spec.withColumn(col_name, \"string\", values=solution_arch_values, nullable=nullable)\n",
    "    elif col_type.typeName() == \"string\":\n",
    "        spec = spec.withColumn(col_name, \"string\", minValue=5, maxValue=20, nullable=nullable)\n",
    "    elif col_type.typeName() == \"integer\":\n",
    "        spec = spec.withColumn(col_name, \"int\", minValue=0, maxValue=10000, nullable=nullable)\n",
    "    elif col_type.typeName() == \"long\":\n",
    "        spec = spec.withColumn(col_name, \"long\", minValue=0, maxValue=100000, nullable=nullable)\n",
    "    elif col_type.typeName() == \"double\":\n",
    "        spec = spec.withColumn(col_name, \"double\", minValue=0.0, maxValue=10000.0, nullable=nullable)\n",
    "    elif col_type.typeName() == \"boolean\":\n",
    "        spec = spec.withColumn(col_name, \"boolean\", nullable=nullable)\n",
    "    elif col_type.typeName() == \"timestamp\":\n",
    "        if col_name in date_ranges:\n",
    "            begin = str(date_ranges[col_name][0])\n",
    "            end = str(date_ranges[col_name][1])\n",
    "            spec = spec.withColumn(col_name, \"timestamp\", begin=begin, end=end, nullable=nullable)\n",
    "        else:\n",
    "            spec = spec.withColumn(col_name, \"timestamp\", begin=\"2020-01-01 00:00:00\", end=\"2025-12-31 23:59:59\", nullable=nullable)\n",
    "    elif col_type.typeName() == \"date\":\n",
    "        if col_name in date_ranges:\n",
    "            begin = str(date_ranges[col_name][0])\n",
    "            end = str(date_ranges[col_name][1])\n",
    "            spec = spec.withColumn(col_name, \"date\", begin=begin, end=end, nullable=nullable)\n",
    "        else:\n",
    "            spec = spec.withColumn(col_name, \"date\", begin=\"2020-01-01\", end=\"2025-12-31\", nullable=nullable)\n",
    "    else:\n",
    "        spec = spec.withColumn(col_name, \"string\", minValue=5, maxValue=20, nullable=nullable)\n",
    "\n",
    "synthetic_df = spec.build()\n",
    "\n",
    "display(synthetic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cefd30b-6cda-482a-92a5-9ab688e0a25f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(synthetic_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98516c67-0f55-4076-a831-81eecc1f916d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "synthetic_df.write.mode(\"overwrite\").saveAsTable(\"users.luis_herrera.synthetic_core_usecase_curated\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "sandbox - generatic synthethic data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

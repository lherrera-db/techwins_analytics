{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c92c95c-1d11-4715-9fa3-4e10b32f2e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Subregion Slippage / Acceleration analysis (for SLM/FLM coaching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47bb692-ce63-4c2e-beb7-9f20f692f7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Author: Luis Herrera (luis.herrera@databricks.com) \n",
    "- Version: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f356fc-8b37-4df4-bf94-c1b697c260d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This notebook is designed for subregion-wide risk and acceleration analysis of of an org opportunities/ use cases, primarily to support SLM/FLM (Sales/Field Leadership) coaching and pipeline management. It leverages both data engineering and AI-powered insights to identify at-risk opportunities and recommend actions for acceleration.\n",
    "\n",
    "**Key Features and Workflow:**\n",
    "\n",
    "- Data Loading & Filtering:\n",
    "The notebook loads curated use case data from core_usecase_curated and applies subregion filters (e.g., SEMEA) to focus the analysis.\n",
    "- Risk & Confidence Scoring:\n",
    "It uses AI models to score each use caseâ€™s likelihood of conversion, extract top risk drivers, and recommend actions (Proceed, Pivot, Walk) with explanations.\n",
    "-Slippage & Acceleration Analysis:\n",
    "The notebook identifies use cases likely to slip in the next 6 months (low confidence, near-term go-live) and quantifies the $DBU at risk, both in total and by field manager.\n",
    "- Acceleration Opportunities:\n",
    "For each use case, it generates AI-driven recommendations to accelerate progress, classifies them into categories (e.g., Funding, Stakeholder Engagement), and aggregates potential $DBU impact by manager and category.\n",
    "- Visualization:\n",
    "Results are displayed as interactive tables, supporting further dashboarding and reporting.\n",
    "\n",
    "This workflow enables proactive pipeline management, helping leadership focus on high-impact interventions and track progress across teams and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f658ceb7-98d7-42f2-8595-39bfc8c866bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the execution timeout to 4 hours (14400 seconds)\n",
    "# Note: On serverless notebooks, the default timeout is 2.5 hours. \n",
    "# You can manually set a longer timeout (e.g., 4 hours) using this config, \n",
    "# but actual enforcement may depend on workspace and cluster policies.\n",
    "spark.conf.set(\"spark.databricks.execution.timeout\", \"14400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f154fb8-1387-4c2f-962b-5eed3a3f5803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Define the fiscal year 2026 start and end dates\n",
    "fy26_start_date = lit(\"2025-02-01\")\n",
    "fy26_end_date = lit(\"2026-01-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62b25c82-fa8b-4bc3-af0d-793397346fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def subregion_wide_df(df, subregion1, subregion2, subregion3): \n",
    "    # Check the values of subregions are not None\n",
    "    filters = []\n",
    "    if subregion1 is not None:\n",
    "        filters.append(col(\"sales_subregion_level_1\") == subregion1)\n",
    "    if subregion2 is not None:\n",
    "        filters.append(col(\"sales_subregion_level_2\") == subregion2)\n",
    "    if subregion3 is not None:\n",
    "        filters.append(col(\"sales_subregion_level_3\") == subregion3)\n",
    "    if not filters:\n",
    "        # if nothing is provided, default to looking at EMEA (note: applying NULL bug filter to catch all EMEA)\n",
    "        filters.append((col(\"sales_region\") == 'EMEA') | (col('business_unit') == 'EMEA') | (col('field_leader') == 'Toby Balfre'))\n",
    "\n",
    "    combined_filter = filters[0]\n",
    "    for f in filters[1:]:\n",
    "        combined_filter = combined_filter & f\n",
    "    \n",
    "    # Filter the DataFrame for usecases created or still open in FY26\n",
    "    return df.filter(\n",
    "        (\n",
    "            combined_filter\n",
    "        ) & (\n",
    "            # and we want use cases modified in FY26\n",
    "            (col(\"last_modified_date\") >= fy26_start_date) & (col(\"last_modified_date\") <= fy26_end_date)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09105ba7-01fa-4f7a-b025-757cac5baf0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set subregion filters for data processing.\n",
    "# These variables can be used to filter data by subregion as needed.\n",
    "\n",
    "subregion1 = None\n",
    "subregion2 = None\n",
    "subregion3 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef49d22a-2049-489c-9a38-da33d46e71ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "[All Usecases] Risk Assessment Dataset"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the columns to select from the dataframe required for the confidence analsis\n",
    "cols = [\n",
    "    \"snapshot_date\", \n",
    "    \"field_manager\",\n",
    "    \"account_name\", \n",
    "    \"usecase_name\", \n",
    "    \"account_executive\",\n",
    "    \"account_solution_architect\",\n",
    "    \"stage\",\n",
    "    \"target_live_date\",\n",
    "    \"onboarding_date\",\n",
    "    \"estimated_monthly_dollar_dbus\", \n",
    "    \"usecase_description\",\n",
    "    \"demand_plan_stage_next_steps\", \n",
    "    \"implementation_notes\",\n",
    "    \"sales_subregion_level_1\",\n",
    "    \"sales_subregion_level_2\",\n",
    "    \"sales_subregion_level_3\"\n",
    "]\n",
    "\n",
    "# Load the curated use case data from the main table\n",
    "usecase_curated_df = spark.table(\"main.gtm_data.core_usecase_curated\")\n",
    "\n",
    "# Transform the dataframe to a wide format based on subregion filters\n",
    "full_usecase_curated_df = subregion_wide_df(usecase_curated_df,\n",
    "                                            subregion1, \n",
    "                                            subregion2, \n",
    "                                            subregion3)\n",
    "\n",
    "# Select only the specified columns for further analysis\n",
    "full_usecase_curated_df = full_usecase_curated_df.select(cols)\n",
    "\n",
    "display(full_usecase_curated_df.limit(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc53604-ca05-4b9f-bb89-804c0bedcf84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Confidence Scoring of Use Cases & Recommended Actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f08fd3-bab0-4c41-b0ec-99f98f74e106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_date, add_months, expr, trim, regexp_extract, split, when\n",
    "\n",
    "# Helper function to escape single quotes and newlines in AI prompts\n",
    "def escape_prompt(prompt):\n",
    "    return prompt.replace(\"'\", \"''\").replace(\"\\n\", \" \")\n",
    "\n",
    "# Filter and prepare the use case dataframe for confidence analysis\n",
    "usecases_goinglive_next = (\n",
    "    full_usecase_curated_df\n",
    "    .select(*cols)  # Select relevant columns\n",
    "    # Fill missing field_manager and account_solution_architect with default values\n",
    "    .withColumn(\"field_manager\", when(col(\"field_manager\").isNull(), \"FLM not assigned\").otherwise(col(\"field_manager\")))\n",
    "    .withColumn(\"account_solution_architect\", when(col(\"account_solution_architect\").isNull(), \"SA not assigned\").otherwise(col(\"account_solution_architect\")))\n",
    "    # Apply business filters:\n",
    "    # - Only use cases with estimated DBUs >= 5000\n",
    "    # - Exclude certain sales stages\n",
    "    # - Exclude use cases related to Unity Catalog, UC, or Governance\n",
    "    # - Only include use cases with target live date in the next 6 months\n",
    "    .filter(\n",
    "        (col(\"estimated_monthly_dollar_dbus\") >= 5000) &\n",
    "        (~col(\"stage\").isin([\"U1\", \"Lost\", \"Closed\", \"Disqualified\"])) &\n",
    "        (~col(\"usecase_name\").contains(\"Unity Catalog\")) & \n",
    "        (~col(\"usecase_name\").contains(\"UC\")) & \n",
    "        (~col(\"usecase_name\").contains(\"Governance\")) &\n",
    "        (col(\"target_live_date\").between(current_date(), add_months(current_date(), 6)))\n",
    "    )\n",
    "    # Order by estimated DBUs descending\n",
    "    .orderBy(col(\"estimated_monthly_dbus\").desc())\n",
    ")\n",
    "display(usecases_goinglive_next.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4590875c-3914-4fed-b0f3-d49f1e5ea544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "usecases_goinglive_next = usecases_goinglive_next.withColumn(\n",
    "    \"unique_id\", monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "usecases_goinglive_next.count()  # Force materialization and unique_id assignment\n",
    "\n",
    "display(usecases_goinglive_next.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4362cd1-58bf-46c4-a415-71dc7e2ec15c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, ceil, lit, col, expr\n",
    "\n",
    "\n",
    "# Applies a Spark SQL expression to a DataFrame in batches, adding the result as a new column.\n",
    "# This is useful for AI functions with batch limits but also to improve performance\n",
    "\n",
    "\n",
    "def batch_apply_expr(\n",
    "    df,\n",
    "    field_to_add: str,\n",
    "    expr_str: str,\n",
    "    batch_size: int = 100,\n",
    "):\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.window import Window\n",
    "\n",
    "    if \"unique_id\" not in df.columns:\n",
    "        raise ValueError(\"unique_id column is required\")\n",
    "\n",
    "    # Create a stable row index without a global sort: order within hash partitions by unique_id\n",
    "    w = Window.partitionBy(F.spark_partition_id()).orderBy(\"unique_id\")\n",
    "    df_idx = df.withColumn(\"_row_num\", F.row_number().over(w) - F.lit(1))\n",
    "\n",
    "    # Derive a synthetic batch id; then range-repartition to keep partitions â‰ˆ batch_size\n",
    "    df_tagged = df_idx.withColumn(\"_batch_id\", (F.col(\"_row_num\") / F.lit(batch_size)).cast(\"int\"))\n",
    "    df_part = df_tagged.repartitionByRange(F.col(\"_batch_id\"))\n",
    "\n",
    "    # Column pruning: keep only columns needed for the expression + unique_id + housekeeping\n",
    "    # Heuristic: extract candidate column names mentioned in expr_str; fall back to keeping all.\n",
    "    import re as _re\n",
    "    mentioned = set(_re.findall(r\"\\b([A-Za-z_][A-Za-z0-9_]*)\\b\", expr_str))\n",
    "    keep = [c for c in df_part.columns if c in (mentioned | {\"unique_id\",\"_row_num\",\"_batch_id\"})] or df_part.columns\n",
    "    slim = df_part.select(*keep)\n",
    "\n",
    "    # Apply the expression in a single pass\n",
    "    out = slim.withColumn(field_to_add, F.expr(expr_str))\n",
    "\n",
    "    # Join back to any dropped columns by unique_id if we pruned aggressively\n",
    "    if set(out.columns) != set(df_part.columns + [field_to_add]):\n",
    "        out = out.join(df_part.drop(*[c for c in df_part.columns if c in out.columns and c != \"unique_id\"]), on=\"unique_id\", how=\"left\")\n",
    "\n",
    "    return out.drop(\"_row_num\",\"_batch_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb832291-d51b-4d37-8108-ed627005f650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temp dataframe with only the fields needed for the one-liner calculation\n",
    "temp_df = usecases_goinglive_next.select(\n",
    "    \"unique_id\",\n",
    "    \"usecase_name\",\n",
    "    \"usecase_description\",\n",
    "    \"demand_plan_stage_next_steps\",\n",
    "    \"implementation_notes\",\n",
    ")\n",
    "\n",
    "# Generate a one-liner summary for each use case using AI summarization\n",
    "temp_df = batch_apply_expr(\n",
    "    temp_df,\n",
    "    \"one_liner\",\n",
    "    \"ai_summarize(concat_ws(' ', usecase_name, usecase_description, demand_plan_stage_next_steps, implementation_notes))\",\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "# Join the one_liner result back to the main dataframe using unique_id\n",
    "usecases_goinglive_next = usecases_goinglive_next.join(\n",
    "    temp_df.select(\"unique_id\", \"one_liner\"),\n",
    "    on=\"unique_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "usecases_goinglive_next.count()  # Force materialization\n",
    "\n",
    "# Display the field manager, account name, use case name, and the generated one-liner summary\n",
    "display(\n",
    "    usecases_goinglive_next\n",
    "        .orderBy(col(\"estimated_monthly_dollar_dbus\").desc())\n",
    "         .select(\n",
    "            \"field_manager\",\n",
    "            \"account_name\",\n",
    "            \"usecase_name\",\n",
    "            \"estimated_monthly_dollar_dbus\",\n",
    "            \"one_liner\"\n",
    "        ).limit(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32463254-68e8-44f7-877b-6fdf03b99ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the confidence prompt outside the expression for clarity\n",
    "ai_prompt_confidence = escape_prompt(\n",
    "    \"Using the MEDDPICC framework, and the following context, output a confidence score (0-100) for successful go-live. Only output the number that is multiple of 10.  Context: \"\n",
    ")\n",
    "\n",
    "# Create a temp dataframe with only the fields needed to calculate confidence\n",
    "temp_conf_df = usecases_goinglive_next.select(\n",
    "    \"unique_id\",\n",
    "    \"usecase_description\",\n",
    "    \"demand_plan_stage_next_steps\",\n",
    "    \"implementation_notes\"\n",
    ")\n",
    "\n",
    "# Use batch_apply_expr to batch the ai_query calls for confidence calculation\n",
    "temp_conf_df = batch_apply_expr(\n",
    "    temp_conf_df,\n",
    "    \"confidence\",\n",
    "    f\"ai_query('databricks-gpt-oss-120b', concat('{ai_prompt_confidence}', coalesce(trim(usecase_description),''), ' ', coalesce(trim(demand_plan_stage_next_steps),''), ' ', coalesce(trim(implementation_notes),'')))\",\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "# Join the calculated confidence back to usecases_goinglive_next\n",
    "usecases_goinglive_next = usecases_goinglive_next.join(\n",
    "    temp_conf_df.select(\"unique_id\", \"confidence\"),\n",
    "    on=\"unique_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "usecases_goinglive_next.count()  # Force materialization\n",
    "\n",
    "display(\n",
    "    usecases_goinglive_next\n",
    "        .orderBy(col(\"estimated_monthly_dollar_dbus\").desc())\n",
    "        .select(\n",
    "            \"field_manager\",\n",
    "            \"account_name\",\n",
    "           \"usecase_name\",\n",
    "            \"estimated_monthly_dollar_dbus\",\n",
    "            \"one_liner\",\n",
    "            \"confidence\"\n",
    "        ).limit(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60c181c-c511-45b1-83a6-7eed91d76796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate an AI prompt for extracting the top 3 confidence drivers using the MEDDPICC framework\n",
    "ai_prompt_top3drivers = escape_prompt(\n",
    "    \"Using the MEDDPICC framework, and given the use case context, output a string that lists the top 3 drivers (factors raising or lowering confidence). Present them in the format: 1. <driver1> ||| 2. <driver2> ||| 3. <driver3> |||. Each driver should include a short, executive-level explanation. Use plain text. Context: \"\n",
    ")\n",
    "\n",
    "# Create a temp dataframe with only the fields needed for the top3drivers calculation\n",
    "temp_top3drivers_df = usecases_goinglive_next.select(\n",
    "    \"unique_id\",\n",
    "    \"usecase_description\",\n",
    "    \"demand_plan_stage_next_steps\",\n",
    "    \"implementation_notes\"\n",
    ")\n",
    "\n",
    "# Apply the AI model in batches to generate the top 3 drivers for each use case\n",
    "temp_top3drivers_df = batch_apply_expr(\n",
    "    temp_top3drivers_df,\n",
    "    \"top3drivers\",\n",
    "    f\"\"\"ai_query('databricks-gpt-oss-120b', concat('{ai_prompt_top3drivers}', concat_ws(' ', usecase_description, demand_plan_stage_next_steps, implementation_notes)))\"\"\",\n",
    "    batch_size=100\n",
    ")\n",
    "# Display the results with the top 3 drivers included\n",
    "display(temp_top3drivers_df.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d91c54-5735-4a85-8e8e-52cd45ff9142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the top 3 confidence drivers from the 'top3drivers' string using regex and trim whitespace\n",
    "from pyspark.sql.functions import col, trim, regexp_extract, when\n",
    "\n",
    "temp_top3drivers_df = temp_top3drivers_df.withColumn(\n",
    "    \"driver_1\", trim(regexp_extract(col(\"top3drivers\"), r\"1\\.\\s*(.*?)\\s*\\|\\|\\|\", 1))\n",
    ").withColumn(\n",
    "    \"driver_2\", trim(regexp_extract(col(\"top3drivers\"), r\"2\\.\\s*(.*?)\\s*\\|\\|\\|\", 1))\n",
    ").withColumn(\n",
    "    \"driver_3\", trim(regexp_extract(col(\"top3drivers\"), r\"3\\.\\s*(.*?)\\s*\\|\\|\\|\", 1))\n",
    ")\n",
    "\n",
    "# Display the field manager, use case name, confidence score, and the extracted top 3 drivers\n",
    "display(temp_top3drivers_df.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8cacef-d406-4019-b304-fd4197f399d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usecases_goinglive_next = usecases_goinglive_next.join(\n",
    "    temp_top3drivers_df.select(\"unique_id\", \"driver_1\", \"driver_2\", \"driver_3\"),\n",
    "    on=\"unique_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "usecases_goinglive_next.count()  # Force materialization\n",
    "\n",
    "display(\n",
    "   usecases_goinglive_next\n",
    "       .orderBy(col(\"estimated_monthly_dollar_dbus\").desc())\n",
    "       .select(\n",
    "            \"field_manager\",\n",
    "            \"account_name\",\n",
    "            \"usecase_name\",\n",
    "            \"estimated_monthly_dollar_dbus\",\n",
    "            \"confidence\",\n",
    "            \"driver_1\",\n",
    "            \"driver_2\",\n",
    "            \"driver_3\"\n",
    "        ).limit(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381841b2-da04-41ea-91ab-2b227c282664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, trim, regexp_extract, col, expr\n",
    "\n",
    "# Define the AI prompt for recommending an action based on MEDDPICC and confidence score\n",
    "ai_prompt_recommend_action = escape_prompt(\n",
    "\"\"\"Using MEDDPICC and the given confidence score (0â€“100), return exactly one of: \n",
    "Proceed: <brief reason>\n",
    "Pivot: <brief reason>\n",
    "Walk: <brief reason>\n",
    "Only output one line in this format.\n",
    "Dont mention the confidence score or MEDDPICC in the output, just the reason.\n",
    "If you output anything else, regenerate until compliant. \n",
    "Context: \"\"\"\n",
    ")\n",
    "\n",
    "# Create a temp dataframe with only the fields needed for the recommend_action calculation\n",
    "temp_recommend_action_df = usecases_goinglive_next.select(\n",
    "    \"unique_id\",\n",
    "    \"confidence\",\n",
    "    \"usecase_description\",\n",
    "    \"demand_plan_stage_next_steps\",\n",
    "    \"implementation_notes\"\n",
    ")\n",
    "\n",
    "# Apply the AI model in batches to generate a recommended action and explanation for each use case\n",
    "temp_recommend_action_df = batch_apply_expr(\n",
    "    temp_recommend_action_df,\n",
    "    \"recommend_action_full\",\n",
    "    f\"\"\"ai_query('databricks-gpt-oss-120b', concat('{ai_prompt_recommend_action}', ' Confidence: ', cast(confidence as string), '. ', concat_ws(' ', usecase_description, demand_plan_stage_next_steps, implementation_notes)))\"\"\",\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "display(temp_recommend_action_df.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b075aaac-6fb9-4123-b9c6-231228b4f8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the recommended action (Proceed, Pivot, Walk) from the AI output\n",
    "temp_recommend_action_df = temp_recommend_action_df.withColumn(\n",
    "    \"recommend_action\",\n",
    "    regexp_extract(col(\"recommend_action_full\"), r\"^(?i)\\s*(Proceed|Pivot|Walk)\", 1)\n",
    ")\n",
    "# If the action is missing or empty, set it to \"cant tell\"\n",
    "temp_recommend_action_df = temp_recommend_action_df.withColumn(\n",
    "    \"recommend_action\",\n",
    "    when((col(\"recommend_action\").isNull()) | (trim(col(\"recommend_action\")) == \"\"), lit(\"cant tell\")).otherwise(col(\"recommend_action\"))\n",
    ")\n",
    "# Extract the explanation for the recommended action\n",
    "temp_recommend_action_df = temp_recommend_action_df.withColumn(\n",
    "    \"recommend_action_explanation\",\n",
    "    trim(regexp_extract(col(\"recommend_action_full\"), r\"^(?i)(?:Proceed|Pivot|Walk)\\s*:\\s*(.*)$\", 1))\n",
    ")\n",
    "\n",
    "# Join recommend_action and recommend_action_explanation to usecases_goinglive_next by unique_id\n",
    "usecases_goinglive_next = usecases_goinglive_next.join(\n",
    "    temp_recommend_action_df.select(\"unique_id\", \"recommend_action\", \"recommend_action_explanation\"),\n",
    "    on=\"unique_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "usecases_goinglive_next.count()  # Force materialization\n",
    "\n",
    "# Display the relevant columns including the recommended action and its explanation\n",
    "#display(\n",
    "#    usecases_goinglive_next\n",
    "#    .orderBy(col(\"estimated_monthly_dollar_dbus\").desc())\n",
    "#    .select(\n",
    "#        \"field_manager\",\n",
    "#        \"account_name\",\n",
    "#        \"usecase_name\",\n",
    "#        \"estimated_monthly_dollar_dbus\",\n",
    "#        \"recommend_action\",\n",
    "#        \"recommend_action_explanation\"\n",
    "#    )\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5787b6ab-ab10-489a-be02-b08f43ac0bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Columns to persist for the usecases_goinglive_next table\n",
    "cols_to_persist = [\n",
    "    \"snapshot_date\", \n",
    "    \"sales_subregion_level_1\",\n",
    "    \"sales_subregion_level_2\",\n",
    "    \"sales_subregion_level_3\",\n",
    "    \"field_manager\",\n",
    "    \"account_name\", \n",
    "    \"usecase_name\", \n",
    "    \"account_executive\",\n",
    "    \"account_solution_architect\",\n",
    "    \"stage\",\n",
    "    \"target_live_date\",\n",
    "    \"onboarding_date\",\n",
    "    \"estimated_monthly_dollar_dbus\", \n",
    "    \"one_liner\",\n",
    "    \"confidence\",\n",
    "    \"recommend_action\",\n",
    "    \"recommend_action_explanation\",\n",
    "    \"driver_1\",\n",
    "    \"driver_2\",\n",
    "    \"driver_3\",\n",
    "]\n",
    "\n",
    "spark.conf.set(\"spark.databricks.execution.timeout\", \"14400\")\n",
    "\n",
    "# Drop the existing table if it exists before overwriting\n",
    "spark.sql(\"DROP TABLE IF EXISTS users.luis_herrera.usecases_goinglive_next\")\n",
    "\n",
    "# Persist the selected columns to the target table\n",
    "usecases_goinglive_next.select(cols_to_persist) \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"users.luis_herrera.usecases_goinglive_next\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5fb2b1-37ba-42f2-bc2f-5fd643ab4192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"OPTIMIZE users.luis_herrera.usecases_goinglive_next\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939b320d-b5e3-4cf2-8741-c692bffa9f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## # UC likely to slip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b1a894-fd32-4e5a-a398-19763c985a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, expr, current_date, add_months\n",
    "\n",
    "# Select use cases likely to slip: confidence < 80, target_live_date in next 6 months, and recommend_action is Pivot or Proceed\n",
    "slip_candidates_df = usecases_goinglive_next.filter(\n",
    "    (col(\"confidence\") < 80) &\n",
    "    (col(\"target_live_date\").between(current_date(), add_months(current_date(), 6))) &\n",
    "    (trim(col(\"recommend_action\")).isin([\"Pivot\", \"Proceed\"]))\n",
    ")\n",
    "\n",
    "#display(\n",
    "#    slip_candidates_df\n",
    "#    .orderBy(col(\"estimated_monthly_dollar_dbus\").desc())\n",
    "#    .select(\n",
    "#        \"field_manager\",\n",
    "#        \"usecase_name\",\n",
    "#        \"confidence\",\n",
    "#        \"target_live_date\",\n",
    "#        \"estimated_monthly_dollar_dbus\",\n",
    "#        \"recommend_action_explanation\"\n",
    "#    ).limit(15)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1723a45e-b30f-4536-b003-68dedc8d28ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define slip categories and their descriptions for AI classification\n",
    "slip_categories = [\n",
    "    (\"Technical\", \"Use cases may slip due to technical limitations, blockers, or unresolved challenges such as architectural complexity, lack of required features, or unresolved bugs that impede progress.\"),\n",
    "    (\"Business\", \"Business priorities, shifting organizational focus, lack of executive sponsorship, or unclear value proposition can deprioritize or stall use cases.\"),\n",
    "    (\"Stakeholder\", \"Insufficient stakeholder engagement, lack of buy-in, misalignment between teams, or changes in key personnel can delay or derail use case progress.\"),\n",
    "    (\"Budget\", \"Budget constraints, delayed funding approvals, or unexpected cost overruns can prevent necessary resources from being allocated, causing slippage.\"),\n",
    "    (\"Project Timelines\", \"Aggressive or unrealistic project timelines, missed deadlines, or dependencies on other projects can result in schedule slippage.\"),\n",
    "    (\"Data\", \"Delays in data access, poor data quality, data privacy concerns, or complex data integration requirements can block or slow down use case delivery.\"),\n",
    "    (\"Integration\", \"Challenges integrating with existing systems, APIs, or tools, or dependencies on third-party integrations, can introduce delays.\"),\n",
    "    (\"External dependencies\", \"Reliance on third parties, vendors, or external teams for critical deliverables or approvals can introduce uncertainty and delays.\"),\n",
    "    (\"Partner/Hyperscaler\", \"Issues related to technology partners or hyperscalers, such as lack of region support, limited partner capabilities, or changes in partner roadmaps, can impact timelines.\"),\n",
    "    (\"Competition\", \"Competitive pressures, market changes, or actions by competitors may shift priorities or require re-evaluation of the use case, causing delays.\"),\n",
    "    (\"Other\", \"Any other reasons not covered above, including unforeseen events, regulatory changes, or unique customer-specific challenges.\")\n",
    "]\n",
    "array_str = ', '.join([f'\"{c[0]}\"' for c in slip_categories])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be92d3f6-cd2f-4e13-865f-21531f76170b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Persist slip_categories as a table for later use\n",
    "spark.conf.set(\"spark.databricks.execution.timeout\", \"14400\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS users.luis_herrera.slipage_categories\")\n",
    "slip_categories_df = spark.createDataFrame(slip_categories, [\"category\", \"description\"])\n",
    "slip_categories_df.write.mode(\"overwrite\").saveAsTable(\"users.luis_herrera.slipage_categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f276368d-af89-4961-ba41-562aecb3dd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, lit, expr\n",
    "\n",
    "\n",
    "# Create a temp dataframe with only the fields needed for the category calculation\n",
    "temp_slip_category_df = slip_candidates_df.select(\n",
    "    \"unique_id\",\n",
    "    \"recommend_action_explanation\"\n",
    ")\n",
    "\n",
    "# Use batch_apply_expr to calculate the category in batches\n",
    "temp_slip_category_batched_df = batch_apply_expr(\n",
    "    temp_slip_category_df,\n",
    "    \"category\",\n",
    "    f\"ai_classify(recommend_action_explanation, array({array_str}))\",\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "temp_slip_category_batched_df = temp_slip_category_batched_df.withColumn(\n",
    "    \"category\",\n",
    "    when(col(\"category\").isNull() | (trim(col(\"category\")) == \"\"), lit(\"Other\")).otherwise(col(\"category\"))\n",
    ")\n",
    "\n",
    "# Join the category result back to slip_candidates_df\n",
    "slip_candidates_df = slip_candidates_df.join(\n",
    "    temp_slip_category_batched_df.select(\"unique_id\", \"category\"),\n",
    "    on=\"unique_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#display(\n",
    "#    slip_candidates_final_df\n",
    "#    .orderBy(col(\"estimated_monthly_dollar_dbus\").desc())\n",
    "#   .select(\n",
    "#        \"field_manager\",\n",
    "#        \"usecase_name\",\n",
    "#        \"confidence\",\n",
    "#        \"target_live_date\",\n",
    "#        \"estimated_monthly_dollar_dbus\",\n",
    "#        \"category\"\n",
    "#    ).limit(15)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbf891d-832b-4541-8945-46fc3678f638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, trim, coalesce, when, col, lit\n",
    "\n",
    "# AI prompt for slippage recommendation\n",
    "ai_prompt_slippage = (\n",
    "    \"Given the following use case description, next steps, and implementation notes, recommend one specific action to mitigate or eliminate slippage. \"\n",
    "    \"Base your recommendation on exactly one of the following categories: \"\n",
    "    \"Technical, Business, Stakeholder, Budget, Project Timelines, Data, Integration, External dependencies, Partner/Hyperscaler, Competition, Other. \"\n",
    "    \"If no clear mitigation or elimination action is possible, output exactly: No clear way of eliminating or mitigating slippage. \"\n",
    "    \"Only output the recommendation as a concise sentence.\"\n",
    ")\n",
    "\n",
    "# Create a temp dataframe with only the fields needed for the slippage recommendation\n",
    "temp_slip_df = slip_candidates_df.select(\n",
    "    \"unique_id\",\n",
    "    \"usecase_description\",\n",
    "    \"demand_plan_stage_next_steps\",\n",
    "    \"implementation_notes\"\n",
    ")\n",
    "\n",
    "# Use batch_apply_expr to batch the ai_query calls for slippage recommendation\n",
    "temp_slip_batched_df = batch_apply_expr(\n",
    "    temp_slip_df,\n",
    "    \"slippage_recommendation\",\n",
    "    f\"\"\"ai_query('databricks-gpt-oss-120b', \\\n",
    "        concat('{ai_prompt_slippage}', \\\n",
    "               ' Description: ', coalesce(trim(usecase_description),''), \\\n",
    "               ' Next Steps: ',  coalesce(trim(demand_plan_stage_next_steps), ''), \\\n",
    "               ' Implementation Notes: ', coalesce(trim(implementation_notes), '')\\\n",
    "        )\\\n",
    "    )\"\"\",\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "temp_slip_batched_df = temp_slip_batched_df.withColumn(\n",
    "    \"slippage_recommendation\",\n",
    "    when(col(\"slippage_recommendation\").isNull() | (trim(col(\"slippage_recommendation\")) == \"\"), lit(\"No clear way of eliminating or mitigating slippage\")).otherwise(col(\"slippage_recommendation\"))\n",
    ")\n",
    "\n",
    "# Join the slippage recommendation back to slip_candidates_df using unique_id\n",
    "slip_candidates_df = slip_candidates_df.join(\n",
    "    temp_slip_batched_df.select(\"unique_id\", \"slippage_recommendation\"),\n",
    "    on=\"unique_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#display(\n",
    "#    slip_candidates_df.select(\n",
    "#        \"field_manager\",\n",
    "#        \"usecase_name\",\n",
    "#        \"confidence\",\n",
    "#        \"target_live_date\",\n",
    "#        \"category\",\n",
    "#        \"slippage_recommendation\"\n",
    "#    ).limit(15)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313c5062-741e-4204-9476-b491d2e9b923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_for_slippage = [\n",
    "    \"snapshot_date\", \n",
    "    \"sales_subregion_level_1\",\n",
    "    \"sales_subregion_level_2\",\n",
    "    \"sales_subregion_level_3\",\n",
    "    \"field_manager\",\n",
    "    \"account_name\", \n",
    "    \"usecase_name\", \n",
    "    \"account_executive\",\n",
    "    \"account_solution_architect\",\n",
    "    \"stage\",\n",
    "    \"target_live_date\",\n",
    "    \"onboarding_date\",\n",
    "    \"estimated_monthly_dollar_dbus\", \n",
    "    \"one_liner\",\n",
    "    \"category\",\n",
    "    \"slippage_recommendation\"\n",
    "]\n",
    "\n",
    "slip_candidates_final_df = slip_candidates_df.select(cols_for_slippage)\n",
    "#display(slip_candidates_final_df.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16075103-597f-4da7-aae6-965b05860724",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.execution.timeout\", \"14400\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS users.luis_herrera.slip_candidates\")\n",
    "slip_candidates_final_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"false\").saveAsTable(\"users.luis_herrera.slip_candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8dcac3a-9d79-4577-b2cd-09577e9129d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## UC that can be potentially accelerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6768bf-6566-44c2-a1ed-7254d3d55faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, expr, current_date, add_months\n",
    "\n",
    "# Filter use cases with target live date in next 6 months and action is Pivot or Proceed\n",
    "accel_candidates_df = usecases_goinglive_next.filter(\n",
    "    (col(\"target_live_date\").between(current_date(), add_months(current_date(), 6))) &\n",
    "    (trim(col(\"recommend_action\")).isin([\"Pivot\", \"Proceed\"]))\n",
    ")\n",
    "# Display relevant columns for acceleration candidates\n",
    "#display(\n",
    "#    accel_candidates_df.select(\n",
    "#        \"field_manager\",\n",
    "#        \"usecase_name\",\n",
    "#        \"confidence\",\n",
    "#        \"target_live_date\",\n",
    "#        \"recommend_action\"\n",
    "#    ).limit(15)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f9f52b-a4a0-4fe3-acc0-aabc1fa2a8e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, trim, coalesce\n",
    "\n",
    "# AI prompt for acceleration recommendation\n",
    "ai_prompt_accelerate = (\n",
    "    \"Given the following use case description, next steps and implementation notes, recommend a specific action to accelerate progress. \"\n",
    "    \"Base your recommendation on one of these categories: Funding, Implementation Planning, Resource Allocation, Stakeholder Engagement, \"\n",
    "    \"Technical Optimization, Risk Mitigation, Partnership Leverage, Process Streamlining, Monitoring and Support, Innovation and Tools. \"\n",
    "    \"If you cannot identify a clear acceleration opportunity, output exactly: No clear acceleration opportunity. \"\n",
    "    \"Only output the recommendation as a concise sentence.\"\n",
    ")\n",
    "\n",
    "# Create a temp dataframe with only the fields needed for the acceleration recommendation\n",
    "temp_accel_df = accel_candidates_df.select(\n",
    "    \"unique_id\",\n",
    "    \"usecase_description\",\n",
    "    \"demand_plan_stage_next_steps\",\n",
    "    \"implementation_notes\"\n",
    ")\n",
    "\n",
    "# Use batch_apply_expr to batch the ai_query calls for acceleration recommendation\n",
    "temp_accel_df = batch_apply_expr(\n",
    "    temp_accel_df,\n",
    "    \"acceleration_recommendation\",\n",
    "    f\"\"\"ai_query('databricks-gpt-oss-120b', \\\n",
    "        concat('{ai_prompt_accelerate}', \\\n",
    "               ' Description: ', coalesce(trim(usecase_description),''), \\\n",
    "               ' Next Steps: ',  coalesce(trim(demand_plan_stage_next_steps), ''), \\\n",
    "               ' Implementation Notes: ', coalesce(trim(implementation_notes), '')\\\n",
    "        )\\\n",
    "    )\"\"\",\n",
    "    batch_size=100\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f129f80-6f4f-4f04-9ee4-cc7af27d9c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join the acceleration recommendation back to accel_candidates_df\n",
    "accel_candidates_df = accel_candidates_df.join(\n",
    "    temp_accel_df.select(\"unique_id\", \"acceleration_recommendation\"),\n",
    "    on=\"unique_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#display(\n",
    "#    accel_candidates_df.select(\n",
    "#        \"field_manager\",\n",
    "#        \"usecase_name\",\n",
    "#        \"confidence\",\n",
    "#        \"target_live_date\",\n",
    "#        \"acceleration_recommendation\"\n",
    "#    ).limit(15)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2a0b7b-269b-4eb6-955a-a723d83d83a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define categories and their descriptions for AI classification\n",
    "accel_categories = [\n",
    "    (\"Funding\", \"Accelerate the use case by securing funding, investment, or financial approvals to support project progression.\"),\n",
    "    (\"Implementation Planning\", \"Accelerate the use case by finalizing project plans, setting clear milestones, and establishing a delivery timeline to ensure timely completion.\"),\n",
    "    (\"Resource Allocation\", \"Accelerate the use case by allocating sufficient resources, including personnel, infrastructure, and tools, to support project requirements.\"),\n",
    "    (\"Stakeholder Engagement\", \"Accelerate the use case by engaging with key stakeholders, including customers, partners, and executives, to build momentum, resolve blockers, and ensure alignment.\"),\n",
    "    (\"Technical Optimization\", \"Accelerate the use case by optimizing technical aspects, such as data migration, integration, and performance, to improve overall efficiency and speed.\"),\n",
    "    (\"Risk Mitigation\", \"Accelerate the use case by identifying and mitigating potential risks, including technical debt, skill gaps, and interoperability issues, to ensure project stability and progress.\"),\n",
    "    (\"Partnership Leverage\", \"Accelerate the use case by leveraging partnerships, collaborations, and joint initiatives to access expertise, resources, and funding opportunities.\"),\n",
    "    (\"Process Streamlining\", \"Accelerate the use case by streamlining processes, including customer enablement, sales kits, and account activation, to reduce delays and improve overall velocity.\"),\n",
    "    (\"Monitoring and Support\", \"Accelerate the use case by providing regular monitoring, support, and feedback to ensure project progress, identify roadblocks, and address issues promptly.\"),\n",
    "    (\"Innovation and Tools\", \"Accelerate the use case by leveraging innovative solutions, tools, and technologies, such as AI, Lakebridge, and Tredence's migration accelerator, to expedite project delivery and improve outcomes.\")\n",
    "]\n",
    "array_str = ', '.join([f'\"{c[0]}\"' for c in accel_categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb25360f-6bd5-40b1-a819-1e8a48cd4a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS users.luis_herrera.acceleration_categories\")\n",
    "accel_categories_df = spark.createDataFrame(accel_categories, [\"category\", \"description\"])\n",
    "accel_categories_df.write.mode(\"overwrite\").saveAsTable(\"users.luis_herrera.acceleration_categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24a76be-f954-42c3-bbe2-02d69d9b7014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Classify the AI-generated recommendation into one of the defined categories\n",
    "acceleration_final_df =  accel_candidates_df.withColumn(\n",
    "        \"category\",\n",
    "        expr(f\"ai_classify(acceleration_recommendation, array({array_str}))\")\n",
    "      )\n",
    "# display(acceleration_final_df\n",
    "#     .orderBy(col(\"estimated_monthly_dollar_dbus\").desc())\n",
    "#     .select(\n",
    "#         \"field_manager\",\n",
    "#         \"usecase_name\",\n",
    "#         \"confidence\",\n",
    "#         \"target_live_date\",\n",
    "#         \"estimated_monthly_dollar_dbus\",\n",
    "#         \"category\",\n",
    "#         \"acceleration_recommendation\"\n",
    "#     ).limit(15)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ccecf63-9eb5-4bb9-8d8f-a4fa6908d451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_for_accel = [\n",
    "    \"snapshot_date\", \n",
    "    \"sales_subregion_level_1\",\n",
    "    \"sales_subregion_level_2\",\n",
    "    \"sales_subregion_level_3\",\n",
    "    \"field_manager\",\n",
    "    \"account_name\", \n",
    "    \"usecase_name\", \n",
    "    \"account_executive\",\n",
    "    \"account_solution_architect\",\n",
    "    \"stage\",\n",
    "    \"target_live_date\",\n",
    "    \"onboarding_date\",\n",
    "    \"estimated_monthly_dollar_dbus\", \n",
    "    \"one_liner\",\n",
    "    \"category\",\n",
    "    \"acceleration_recommendation\"   \n",
    "]\n",
    "\n",
    "\n",
    "acceleration_final_df = acceleration_final_df.select(cols_for_accel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52caa7a-d9d2-470d-b007-7e04aa4efa28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.execution.timeout\", \"14400\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS users.luis_herrera.acceleration_candidates\")\n",
    "acceleration_final_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"false\").saveAsTable(\"users.luis_herrera.acceleration_candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741d8692-b658-4d2e-ab3a-891db70533fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Document the table and fields using table and column comments to later use within Genie\n",
    "\n",
    "# Table comment for usecases_goinglive_next\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE users.luis_herrera.usecases_goinglive_next\n",
    "SET TBLPROPERTIES (\n",
    "  'comment' = 'Table containing use cases expected to go live within next 6 monthns, with MEDDPICC-based AI recommendations and related metadata.'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Column comments for usecases_goinglive_next\n",
    "column_comments = {\n",
    "    \"snapshot_date\": \"Date of salesforce data snapshot.\",\n",
    "    \"sales_subregion_level_1\": \"Top-level sales subregion. Possible values: Central, BNLX & Nordics, UKI, SEMEA, Emerging, Greenfield, DNB, FINS, HLS.\",\n",
    "    \"sales_subregion_level_2\": \"Second-level sales subregion. Possible values: Germany Enterprise, Germany Strategic Non-Regulated, Switzerland, Benelux_Non Regulated, Germany Strategic Regulated, Nordics, UK TTL, MEI & CME, UK HLS, PBS & RCG, Spain, IMEA, France Product Industries, UK FSI & Public Sector, Benelux_Regulated, UK Geo Strat & Named Core, UK&I, France Service Industries, Nordics & Benelux, Central, Italy, Emerging_Greenfield, DNB Hunter, Southern, Nordics_Region, Enterprise_Greenfield_Northern, Emerging Greenfield, France Hunter, Greenfield, FINS ProServ & FinTech SLM, HLS LS.\",\n",
    "    \"sales_subregion_level_3\": \"Third-level sales subregion.\",\n",
    "    \"field_manager\": \"Field manager responsible for the account.\",\n",
    "    \"account_name\": \"Customer account name.\",\n",
    "    \"usecase_name\": \"Name of the use case.\",\n",
    "    \"account_executive\": \"Account executive assigned.\",\n",
    "    \"account_solution_architect\": \"Solution architect assigned.\",\n",
    "    \"stage\": \"Current sales or implementation stage.\",\n",
    "    \"target_live_date\" : \"Planned go-live date for the use case.\",\n",
    "    \"onboarding_date\" : \"Planned onboarding date for the use case\",\n",
    "    \"estimated_monthly_dollar_dbus\": \"Estimated monthly DBUs in dollars.\",\n",
    "    \"one_liner\": \"Brief summary of the use case.\",\n",
    "    \"confidence\": \"AI-assigned confidence score (0-100) for go-live.\",\n",
    "    \"recommend_action\": \"AI-recommended action: Proceed, Pivot, Walk, or cant tell.\",\n",
    "    \"recommend_action_explanation\": \"Explanation for the recommended action.\",\n",
    "    \"driver_1\": \"Primary business or technical driver.\",\n",
    "    \"driver_2\": \"Secondary business or technical driver.\",\n",
    "    \"driver_3\": \"Tertiary business or technical driver.\"\n",
    "}\n",
    "\n",
    "for col_name, comment in column_comments.items():\n",
    "    spark.sql(f\"\"\"\n",
    "    ALTER TABLE users.luis_herrera.usecases_goinglive_next\n",
    "    ALTER COLUMN {col_name} COMMENT '{comment}'\n",
    "    \"\"\")\n",
    "\n",
    "# Table comment for slip_candidates\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE users.luis_herrera.slip_candidates\n",
    "SET TBLPROPERTIES (\n",
    "  'comment' = 'Use cases at risk of slippage within next 6 monthn, with AI-classified slippage category and mitigation recommendations.'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Column comments for slip_candidates\n",
    "slip_column_comments = {\n",
    "    \"snapshot_date\": \"Date of data snapshot.\",\n",
    "    \"sales_subregion_level_1\": \"Top-level sales subregion. Possible values: Central, BNLX & Nordics, UKI, SEMEA, Emerging, Greenfield, DNB, FINS, HLS.\",\n",
    "    \"sales_subregion_level_2\": \"Second-level sales subregion. Possible values: Germany Enterprise, Germany Strategic Non-Regulated, Switzerland, Benelux_Non Regulated, Germany Strategic Regulated, Nordics, UK TTL, MEI & CME, UK HLS, PBS & RCG, Spain, IMEA, France Product Industries, UK FSI & Public Sector, Benelux_Regulated, UK Geo Strat & Named Core, UK&I, France Service Industries, Nordics & Benelux, Central, Italy, Emerging_Greenfield, DNB Hunter, Southern, Nordics_Region, Enterprise_Greenfield_Northern, Emerging Greenfield, France Hunter, Greenfield, FINS ProServ & FinTech SLM, HLS LS.\",\n",
    "    \"sales_subregion_level_3\": \"Third-level sales subregion.\",\n",
    "    \"field_manager\": \"Field manager responsible for the account.\",\n",
    "    \"account_name\": \"Customer account name.\",\n",
    "    \"usecase_name\": \"Name of the use case.\",\n",
    "    \"account_executive\": \"Account executive assigned.\",\n",
    "    \"account_solution_architect\": \"Solution architect assigned.\",\n",
    "    \"stage\": \"Current sales or implementation stage.\",\n",
    "    \"target_live_date\": \"Planned go-live date for the use case.\",\n",
    "    \"estimated_monthly_dollar_dbus\": \"Estimated monthly DBUs in dollars.\",\n",
    "    \"one_liner\": \"Brief summary of the use case.\",\n",
    "    \"category\": \"AI-classified slippage category.\",\n",
    "    \"slippage_recommendation\": \"AI-generated recommendation to mitigate or eliminate slippage.\"\n",
    "}\n",
    "\n",
    "for col_name, comment in slip_column_comments.items():\n",
    "    spark.sql(f\"\"\"\n",
    "    ALTER TABLE users.luis_herrera.slip_candidates\n",
    "    ALTER COLUMN {col_name} COMMENT '{comment}'\n",
    "    \"\"\")\n",
    "\n",
    "# Table comment for acceleration_candidates\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE users.luis_herrera.acceleration_candidates\n",
    "SET TBLPROPERTIES (\n",
    "  'comment' = 'Use cases with acceleration opportunities within next 6 monthn, including AI-generated recommendations and classified categories.'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Column comments for acceleration_candidates\n",
    "accel_column_comments = {\n",
    "    \"snapshot_date\": \"Date of data snapshot.\",\n",
    "    \"sales_subregion_level_1\": \"Top-level sales subregion. Possible values: Central, BNLX & Nordics, UKI, SEMEA, Emerging, Greenfield, DNB, FINS, HLS.\",\n",
    "    \"sales_subregion_level_2\": \"Second-level sales subregion. Possible values: Germany Enterprise, Germany Strategic Non-Regulated, Switzerland, Benelux_Non Regulated, Germany Strategic Regulated, Nordics, UK TTL, MEI & CME, UK HLS, PBS & RCG, Spain, IMEA, France Product Industries, UK FSI & Public Sector, Benelux_Regulated, UK Geo Strat & Named Core, UK&I, France Service Industries, Nordics & Benelux, Central, Italy, Emerging_Greenfield, DNB Hunter, Southern, Nordics_Region, Enterprise_Greenfield_Northern, Emerging Greenfield, France Hunter, Greenfield, FINS ProServ & FinTech SLM, HLS LS.\",\n",
    "    \"sales_subregion_level_3\": \"Third-level sales subregion.\",\n",
    "    \"field_manager\": \"Field manager responsible for the account.\",\n",
    "    \"account_name\": \"Customer account name.\",\n",
    "    \"usecase_name\": \"Name of the use case.\",\n",
    "    \"account_executive\": \"Account executive assigned.\",\n",
    "    \"account_solution_architect\": \"Solution architect assigned.\",\n",
    "    \"stage\": \"Current sales or implementation stage.\",\n",
    "    \"target_live_date\": \"Planned go-live date for the use case.\",\n",
    "    \"estimated_monthly_dollar_dbus\": \"Estimated monthly DBUs in dollars.\",\n",
    "    \"one_liner\": \"Brief summary of the use case.\",\n",
    "    \"category\": \"AI-classified acceleration category.\",\n",
    "    \"acceleration_recommendation\": \"AI-generated recommendation to accelerate progress.\"\n",
    "}\n",
    "\n",
    "for col_name, comment in accel_column_comments.items():\n",
    "    spark.sql(f\"\"\"\n",
    "    ALTER TABLE users.luis_herrera.acceleration_candidates\n",
    "    ALTER COLUMN {col_name} COMMENT '{comment}'\n",
    "    \"\"\")\n",
    "\n",
    "# Table comment for slip_categories\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE users.luis_herrera.slipage_categories\n",
    "SET TBLPROPERTIES (\n",
    "  'comment' = 'Reference table of slippage categories and their descriptions for AI classification of use case slippage risk.'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Column comments for slip_categories\n",
    "slip_cat_column_comments = {\n",
    "    \"category\": \"Name of the slippage category.\",\n",
    "    \"description\": \"Description of the slippage category.\"\n",
    "}\n",
    "\n",
    "for col_name, comment in slip_cat_column_comments.items():\n",
    "    spark.sql(f\"\"\"\n",
    "    ALTER TABLE users.luis_herrera.slipage_categories\n",
    "    ALTER COLUMN {col_name} COMMENT '{comment}'\n",
    "    \"\"\")\n",
    "\n",
    "# Table comment for acceleration_categories\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE users.luis_herrera.acceleration_categories\n",
    "SET TBLPROPERTIES (\n",
    "  'comment' = 'Reference table of acceleration categories and their descriptions for AI classification of use case acceleration opportunities.'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Column comments for acceleration_categories\n",
    "accel_cat_column_comments = {\n",
    "    \"category\": \"Name of the acceleration category.\",\n",
    "    \"description\": \"Description of the acceleration category.\"\n",
    "}\n",
    "\n",
    "for col_name, comment in accel_cat_column_comments.items():\n",
    "    spark.sql(f\"\"\"\n",
    "    ALTER TABLE users.luis_herrera.acceleration_categories\n",
    "    ALTER COLUMN {col_name} COMMENT '{comment}'\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "confidence_scoring_batch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
